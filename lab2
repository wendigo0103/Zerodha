import numpy as np

# ---------- Hyperparameters ----------
gamma = 0.99        # discount factor
theta = 1e-6        # small threshold for stopping
max_iterations = 10000

# ---------- Helpers ----------
n_states = env.observation_space.n
n_actions = env.action_space.n

# mapping action index -> human readible
action_map = {0: "Left", 1: "Down", 2: "Right", 3: "Up"}

# Value array
V = np.zeros(n_states)

# Access the MDP model from gym's FrozenLake (env.P)
# env.P[state][action] -> list of (prob, next_state, reward, done, info)
P = env.unwrapped.P

# ---------- Value Iteration ----------
for it in range(max_iterations):
    delta = 0.0
    for s in range(n_states):
        v_old = V[s]
        q_values = np.zeros(n_actions)
        for a in range(n_actions):
            for transition in P[s][a]:
                prob, s_next, reward, done = transition[:4]
                q_values[a] += prob * (reward + gamma * V[s_next])
        V[s] = np.max(q_values)
        delta = max(delta, abs(v_old - V[s]))
    if delta < theta:
        print(f"Value iteration converged in {it+1} iterations (delta={delta:.2e}).")
        break
else:
    print(f"Value iteration stopped after max iterations ({max_iterations}). delta={delta:.2e}")

# ---------- Extract deterministic greedy policy ----------
policy = np.zeros(n_states, dtype=int)  # store best action index per state
for s in range(n_states):
    q_values = np.zeros(n_actions)
    for a in range(n_actions):
        for transition in P[s][a]:
            prob, s_next, reward, done = transition[:4]
            q_values[a] += prob * (reward + gamma * V[s_next])
    best_a = np.argmax(q_values)
    policy[s] = best_a

# ---------- Print results ----------
print("\nEstimated optimal V(s) (4x4):")
print(V.reshape(4, 4))

print("\nGreedy policy (as action names) for each state (0..15):")
policy_actions = [action_map[a] for a in policy]
for r in range(4):
    row = policy_actions[r*4:(r+1)*4]
    print(row)

# Example: rollout from initial state (0) following the greedy policy
def rollout_from(start_state=0, max_steps=50, render=False):
    state = start_state
    # set env to the given state by resetting and stepping until that state is reached
    # simpler approach for gymnasium: use env.reset() and step using policy until you reach start_state
    # but easiest: use env.unwrapped.s = start_state (works for FrozenLake)
    try:
        env.unwrapped.s = start_state
    except Exception:
        # fallback: perform steps from reset
        state, _ = env.reset()
        while state != start_state:
            a = 0  # move left until wraps (cheap fallback; deterministic FrozenLake may reach target)
            state, _, terminated, truncated, _ = env.step(a)
            if terminated or truncated:
                state, _ = env.reset()
        env.unwrapped.s = start_state

    state = start_state
    traj = [state]
    for t in range(max_steps):
        a = policy[state]
        next_state, reward, terminated, truncated, _ = env.step(a)
        traj.append(next_state)
        state = next_state
        if terminated or truncated:
            break
    return traj

print("\nRollout following greedy policy from state 0:")
print(rollout_from(0))
